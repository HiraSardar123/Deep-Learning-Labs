{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW9yd4zRICIb"
      },
      "source": [
        "## Hyperparameter Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXqI7Q8cxt5r"
      },
      "source": [
        "In the first part of our lab, we will explore two methods for hyperparameter tuning a machine learning model.\n",
        "\n",
        "In contrast to model parameters which are learned during training, model hyperparameters are set by the data scientist ahead of training. The weights learned during training are parameters while the number of trees in a random forest is a model hyperparameter because this is set by the data scientist. Hyperparameters can be thought of as model settings. These settings need to be tuned for each problem because the best model hyperparameters for one particular dataset will not be the best across all datasets. The process of hyperparameter tuning (also called hyperparameter optimization) means finding the best combination of hyperparameter values for a machine learning model - as measured on a validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYlPp3QLyUBU"
      },
      "source": [
        "There are several approaches to hyperparameter tuning\n",
        "\n",
        "    1. Manual: Select hyperparameters based on intuition/experience/guessing, train the model with the hyperparameters, and score on the validation data. Repeat this process until you are satisfied with the results.\n",
        "    2. Grid Search: Set up a grid of hyperparameter values and for each combination, train a model and score on the validation data. In this approach, every single combination of hyperparameter values is tried which can be very inefficient.\n",
        "    3. Random search: Set up a grid of hyperparameter values and select random combinations to train the model and score. The number of search iterations is set based on time/resources.\n",
        "    4. Automated Hyperparameter Tuning: Use methods such as gradient descent, Bayesian optimization, or evolutionary algorithms to conduct a guided search for the best combination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtMdaz7iy6EU"
      },
      "source": [
        "In this lab, we will implement approaches 2 and 3 for a Gradient Boosting model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pthc5YRWzFEr"
      },
      "source": [
        "### Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCTFPzU5zIwD"
      },
      "source": [
        "We will work with a subset of the data consisting of 10000 rows. Hyperparameter tuning is extremely computationally expensive and working with the full dataset would not be feasible for more than a few search iterations. However, the same ideas that we will implement here can be applied to the full dataset and while this notebook is specifically aimed at the GBM, the methods can be applied for any machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN581TumzHE0",
        "outputId": "821b5b6c-69a5-4269-c3d8-0d117b8d3511"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Modeling\n",
        "import lightgbm as lgb\n",
        "import logging\n",
        "logging.getLogger(\"lightgbm\").setLevel(logging.CRITICAL)\n",
        "\n",
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "N_FOLDS = 5\n",
        "MAX_EVALS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feuf2Ky2zYPU"
      },
      "source": [
        "Below we read in the data and separate it into a training set of 10000 observations and a \"testing set\" of 6000 observations. After creating the testing set, we cannot do any hyperparameter tuning with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "22MD-oF8zcAL"
      },
      "outputs": [],
      "source": [
        "features = pd.read_csv('/content/data.csv')\n",
        "\n",
        "# Sample 16000 rows (10000 for training, 6000 for testing)\n",
        "features = features.sample(n = 16000, random_state = 42)\n",
        "\n",
        "# Only numeric features\n",
        "features = features.select_dtypes('number')\n",
        "\n",
        "# Extract the labels\n",
        "labels = np.array(features['TARGET'].astype(np.int32)).reshape((-1, ))\n",
        "features = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\n",
        "\n",
        "# Split into training and testing data\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECoqUI6PzfsT",
        "outputId": "cec90077-1faf-4985-f031-202c69cb5719"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training features shape:  (10000, 104)\n",
            "Testing features shape:  (6000, 104)\n"
          ]
        }
      ],
      "source": [
        "print(\"Training features shape: \", train_features.shape)\n",
        "print(\"Testing features shape: \", test_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7l4l76pS1FVr"
      },
      "outputs": [],
      "source": [
        "# Create a training and testing dataset\n",
        "train_set = lgb.Dataset(data = train_features, label = train_labels, free_raw_data=False)\n",
        "test_set = lgb.Dataset(data = test_features, label = test_labels, free_raw_data=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v_pMleEfE1-E"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoBkMIzT1IDL",
        "outputId": "2a9f1801-27c9-4a9d-8e6a-4280fc871de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.135780 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.355919 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076824 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.236686 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.239243 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[19]\tcv_agg's valid auc: 0.716362 + 0.018142\n"
          ]
        }
      ],
      "source": [
        "# Import the early_stopping callback\n",
        "from lightgbm import early_stopping\n",
        "\n",
        "# Get default hyperparameters\n",
        "model = lgb.LGBMClassifier()\n",
        "default_params = model.get_params()\n",
        "\n",
        "# Remove the number of estimators because we set this to 10000 in the cv call\n",
        "del default_params['n_estimators']\n",
        "\n",
        "# Cross validation with early stopping using the callback\n",
        "cv_results = lgb.cv(default_params, train_set, num_boost_round=10000,\n",
        "                    metrics='auc', nfold=N_FOLDS, seed=42,\n",
        "                    callbacks=[early_stopping(stopping_rounds=100)]) # Use early_stopping callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJzeWDp7-Se0",
        "outputId": "9a03c8a2-5358-4d60-c3a7-391c96bc2ed0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'valid auc-mean': [0.6606296820006892, 0.6785737247461092, 0.6876787764186905, 0.6920238536470323, 0.6980282114062015, 0.7007515262272621, 0.7019102402720371, 0.7027799296244843, 0.7038111174147241, 0.7052532594510132, 0.708413098254373, 0.709269070051753, 0.7103137324993294, 0.7106799939939954, 0.7119391930339403, 0.7127252039141159, 0.7144109391747674, 0.7153335805912058, 0.7163621210120483], 'valid auc-stdv': [0.03433995447791942, 0.03923359090275524, 0.034770677423619036, 0.031282692518835725, 0.030364861014553008, 0.032530870027039395, 0.03328890054084283, 0.02812952014701759, 0.02805930888843699, 0.026281552709974957, 0.02366788251994557, 0.022034769091263413, 0.023137377203538416, 0.020539797269945013, 0.02089219775313339, 0.01925231985647043, 0.017883261073009915, 0.017829703218800005, 0.018142016766178933]}\n"
          ]
        }
      ],
      "source": [
        "print(cv_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZaOPLw61I7k",
        "outputId": "11d52ef5-f49b-4fe4-ebb0-617c3dc51aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The maximum validation ROC AUC was: 0.71636 with a standard deviation of 0.01814.\n",
            "The optimal number of boosting rounds (estimators) was 19.\n"
          ]
        }
      ],
      "source": [
        "print('The maximum validation ROC AUC was: {:.5f} with a standard deviation of {:.5f}.'.format(cv_results['valid auc-mean'][-1], cv_results['valid auc-stdv'][-1]))\n",
        "print('The optimal number of boosting rounds (estimators) was {}.'.format(len(cv_results['valid auc-mean'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cgWXsWd1NXk"
      },
      "source": [
        "We can use this result as a baseline model to beat. To find out how well the model does on our \"test\" data, we will retrain it on all the training data with the best number of estimators found during cross validation with early stopping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OOl5Xcx_1Tec"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ykA5JBe1U1W",
        "outputId": "186b5725-0ac2-40ba-9ae1-3766f9982363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 824, number of negative: 9176\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007526 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.082400 -> initscore=-2.410176\n",
            "[LightGBM] [Info] Start training from score -2.410176\n",
            "The baseline model scores 0.71370 ROC AUC on the test set.\n"
          ]
        }
      ],
      "source": [
        "# Optimal number of esimators found in cv\n",
        "model.n_estimators = len(cv_results['valid auc-mean'])\n",
        "\n",
        "# Train and make predicions with model\n",
        "model.fit(train_features, train_labels)\n",
        "preds = model.predict_proba(test_features)[:, 1]\n",
        "baseline_auc = roc_auc_score(test_labels, preds)\n",
        "\n",
        "print('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asgHsumQ1XD1"
      },
      "source": [
        "This is the baseline score before hyperparameter tuning. The only difference we made from the default model was using early stopping to set the number of estimators (which by default is 100)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvgcwq031dZb"
      },
      "source": [
        "### Hyperparameter Tuning Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D_B2Up91h00"
      },
      "source": [
        "Now we have the basic framework in place, we will use cross validation to determine the performance of model hyperparameters and early stopping with the GBM so we do not have to tune the number of estimators. The basic strategy for both grid and random search is simple: for each hyperparameter value combination, evaluate the cross validation score and record the results along with the hyperparameters. Then, at the end of searching, choose the hyperparameters that yielded the highest cross-validation score, train the model on all the training data, and make predictions on the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7ro44v41vXV"
      },
      "source": [
        "It's helpful to think of hyperparameter tuning as having four parts:\n",
        "\n",
        "    1. Objective function: a function that takes in hyperparameters and returns a score we are trying to minimize or maximize\n",
        "    2. Domain: the set of hyperparameter values over which we want to search.\n",
        "    3. Algorithm: method for selecting the next set of hyperparameters to evaluate in the objective function.\n",
        "    4. Results history: data structure containing each set of hyperparameters and the resulting score from the objective function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1o4u_gO2Au8"
      },
      "source": [
        "### Objective Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "isrcMCng1qGc"
      },
      "outputs": [],
      "source": [
        "def objective(hyperparameters, iteration):\n",
        "    \"\"\"Objective function for grid and random search. Returns\n",
        "       the cross validation score from a set of hyperparameters.\"\"\"\n",
        "\n",
        "    # Number of estimators will be found using early stopping\n",
        "    if 'n_estimators' in hyperparameters.keys():\n",
        "        del hyperparameters['n_estimators']\n",
        "\n",
        "    # Perform n_folds cross validation\n",
        "    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round=10000,\n",
        "                    metrics='auc', nfold=N_FOLDS, seed=42,\n",
        "                    callbacks=[early_stopping(stopping_rounds=100)])\n",
        "\n",
        "    # results to retun\n",
        "    score = cv_results['valid auc-mean'][-1]\n",
        "    estimators = len(cv_results['valid auc-mean'])\n",
        "    hyperparameters['n_estimators'] = estimators\n",
        "\n",
        "    return [score, hyperparameters, iteration]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kybgp2aW2FY0",
        "outputId": "f265d760-f957-4666-cd57-6f1f7124e4cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004611 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017504 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004496 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004531 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004389 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[19]\tcv_agg's valid auc: 0.716362 + 0.018142\n",
            "The cross-validation ROC AUC was 0.71636.\n"
          ]
        }
      ],
      "source": [
        "score, params, iteration = objective(default_params, 1)\n",
        "\n",
        "print('The cross-validation ROC AUC was {:.5f}.'.format(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK8rzR_22Gfz"
      },
      "source": [
        "### Domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbRoKu8j2MBk"
      },
      "source": [
        "The domain, or search space, is all the possible values for all the hyperparameters that we want to search over. For random and grid search, the domain is a hyperparameter grid and usually takes the form of a dictionary with the keys being the hyperparameters and the values lists of values for each hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L4IIPMR2HnT",
        "outputId": "a114006d-4820-472f-95e5-4b344a90660b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'boosting_type': 'gbdt',\n",
              " 'class_weight': None,\n",
              " 'colsample_bytree': 1.0,\n",
              " 'importance_type': 'split',\n",
              " 'learning_rate': 0.1,\n",
              " 'max_depth': -1,\n",
              " 'min_child_samples': 20,\n",
              " 'min_child_weight': 0.001,\n",
              " 'min_split_gain': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'num_leaves': 31,\n",
              " 'objective': None,\n",
              " 'random_state': None,\n",
              " 'reg_alpha': 0.0,\n",
              " 'reg_lambda': 0.0,\n",
              " 'subsample': 1.0,\n",
              " 'subsample_for_bin': 200000,\n",
              " 'subsample_freq': 0}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a default model\n",
        "model = lgb.LGBMModel()\n",
        "model.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UoX6oZk2Ut0"
      },
      "source": [
        "Some of these we do not need to tune such as silent, objective, random_state, and n_jobs, and we will use early stopping to determine perhaps the most important hyperparameter, the number of individual learners trained, n_estimators (also referred to as num_boost_rounds or the number of iterations). However, there are still many hyperparameters to optimize, and we will choose 10 to tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Dzy2psu72Ss7"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'boosting_type': ['gbdt', 'goss', 'dart'],\n",
        "    'num_leaves': list(range(20, 150)),\n",
        "    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
        "    'subsample_for_bin': list(range(20000, 300000, 20000)),\n",
        "    'min_child_samples': list(range(20, 500, 5)),\n",
        "    'reg_alpha': list(np.linspace(0, 1)),\n",
        "    'reg_lambda': list(np.linspace(0, 1)),\n",
        "    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
        "    'subsample': list(np.linspace(0.5, 1, 100)),\n",
        "    'is_unbalance': [True, False]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdI48uLR2hIU",
        "outputId": "2a362195-5e70-4a2f-cf84-c14391ee0c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Boosting type:  goss\n",
            "Subsample ratio:  1.0\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.seed(50)\n",
        "\n",
        "# Randomly sample a boosting type\n",
        "boosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n",
        "\n",
        "# Set subsample depending on boosting type\n",
        "subsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n",
        "\n",
        "print('Boosting type: ', boosting_type)\n",
        "print('Subsample ratio: ', subsample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "yQV8hYwK2mej",
        "outputId": "37f6ca13-6dbe-48e1-90da-9a139e94c359"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Learning rate histogram\n",
        "plt.hist(param_grid['learning_rate'], bins = 20, color = 'r', edgecolor = 'k');\n",
        "plt.xlabel('Learning Rate', size = 14); plt.ylabel('Count', size = 14); plt.title('Learning Rate Distribution', size = 18);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjYOUFLW2nHc",
        "outputId": "f267af9c-7fb2-4b65-dac4-bfc62daf89c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 499 values between 0.005 and 0.05\n",
            "There are 499 values between 0.05 and 0.5\n"
          ]
        }
      ],
      "source": [
        "a = 0\n",
        "b = 0\n",
        "\n",
        "# Check number of values in each category\n",
        "for x in param_grid['learning_rate']:\n",
        "    # Check values\n",
        "    if x >= 0.005 and x < 0.05:\n",
        "        a += 1\n",
        "    elif x >= 0.05 and x < 0.5:\n",
        "        b += 1\n",
        "\n",
        "print('There are {} values between 0.005 and 0.05'.format(a))\n",
        "print('There are {} values between 0.05 and 0.5'.format(b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "44nc6gFa2pjj",
        "outputId": "617d8dce-8228-47f1-d314-5f656970d6de"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHRCAYAAACW3ZisAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAS/xJREFUeJzt3XlcFWX///H3YfGAKIgaKgqIZrnmklqmuaSp3ZZLmVbmVrZqWXqbS6GWuZbe1p1pdae2uNXdYtlqLqmp5FpZroWKKa63oCKocP3+8Hfmy5EDAiIHhtfz8TgPZeaamc9cnOXNzHVmHMYYIwAAAJvw8XYBAAAA+YlwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwA1tp3bq1HA6Hxo4d6+1SvCo5OVkxMTGqVauWAgMD5XA45HA4tHXrVm+XhgK0d+9e63e/d+9eb5fjUdWqVeVwODR37txM81y1r1y5ssDryqm5c+fK4XCoatWq3i4FGRBuioGxY8dabxIlS5bUwYMHs2yb8c2wML+hIHs9e/bUyy+/rB07dsjhcKhChQqqUKGC/P39c7S8KyS2bt366haKTFwflpc+SpUqpfDwcDVu3FgDBgzQe++9pzNnzhRobSdPntTYsWM1duxYnTx5skC3XdD27t1r7SuKHsJNMXP27Fm9+OKL3i4DV9GOHTu0ZMkSSdKiRYuUnJyshIQEJSQkqE6dOl6uDrlRvnx5K5iWKFFChw8f1qZNm/Tuu++qX79+qlixol566SVduHDB4/L+/v66/vrrdf311+c42Gbn5MmTevHFF/Xiiy/mW7ipXr26rr/+eoWEhOTL+vLL3r17rX3NTkhIiK6//npVr169gCpDTvh5uwAUvNmzZ2vo0KG67rrrvF0KroLffvtNklSuXDn16NHDy9XgSmzYsMHtdIcxRrt27dLKlSs1Y8YM/fbbbxozZoxWrFihb7/9Vk6n0235ypUra8eOHQVcde4sW7bM2yVckW7duqlbt27eLgOX4MhNMRIREaEbbrhBFy5c0KhRo7xdDq6S5ORkSVKpUqW8XAnym8Ph0PXXX6/HHntMW7Zs0ZAhQyRJK1eu1NNPP+3l6oDCg3BTjPj4+GjixImSpE8++UQ///xzrpbP6eDErAYIXrr8vn379MgjjygyMlIBAQGqXr26XnjhBbdxBNu2bdODDz6oiIgIBQQEqEaNGnr55Zd1/vz5y9Z77tw5TZo0STfccIOCgoIUGhqq22+/Xd98881ll922bZseffRR1ahRQyVLllSpUqV0ww036Pnnn9exY8c8LuMa2+Qap/LJJ5+offv2CgsLk4+PT67P3aekpGj69Om65ZZbFBoaqoCAAEVFRalPnz4eBwa7tt+vXz9J0r59+9zGbLimX22nTp3SpEmT1KxZM5UtW1ZOp1MRERG67777tG7duiyX27Ztm8aOHavbbrtN1atXV2BgoIKDg9WwYUO98MILHvt9y5Yt1v79+uuv2dbVp08fORwOtW3b1uP8r776Svfcc48qV64sp9Op0NBQtWzZUjNnztS5c+eyXO+iRYt0xx13WGOaypQpoxo1aqhz586aMWOGUlJSsq0rr3x9fTV16lR16tRJkvTuu+9q165dbm0u95o9cOCAnn32WdWpU0dBQUFyOp0KDw/XjTfeqGeffVYbNmyw2rZu3VrR0dHWz9HR0W7Pr4zjsy4dZLtixQp17dpVlSpVkq+vr9tzMbsBxRklJCRo0KBBio6OVkBAgCpWrKhevXpleWRq5cqVVm3Z8TTGsGrVqmrTpk2mNp5eSzkZUPznn3/qiSeeUI0aNazndaNGjfTSSy8pKSkpR/Xv2bNHDz30kCIiIuR0OlWlShU98sgj+vvvv7Pdv2LLwPbGjBljJJmoqChjjDGtWrUykkybNm0ytY2LizOSjCSzYsWKLOfFxcVlub2oqCgjycyZMyfL5T/55BNTpkwZI8kEBwcbX19fa96tt95qzp07Z5YsWWJKlixpJJmQkBDjcDisNj179vS4bde+jRw50tx6661GkvHz87O25XqMGTMmy/onT55sfHx8rLYlS5Y0JUqUsH6uVKmS2bx5c5b93KpVKzNkyBAjyTgcDhMaGmp8fX2z3ealDhw4YOrWrWtt09/f34SEhFg/+/j4mNdff91tmVdeecVUqFDBBAcHW20qVKhgPZ5++ukcb9/Vj61atcrxMsYYs2XLFlOlShWrTl9fX1O6dGnrZ4fDYSZMmOBxWdfzRpIJCAgwZcuWdfudV65c2ezYsSPTcnXq1DGSzD//+c8s6zp9+rQJCgoykszcuXPd5iUnJ5vu3bu7PT+Cg4Pdtn3zzTebEydOZFpv//793ZYrVaqU9ZzNyWvFkzlz5uRq2Q0bNljtY2Ji3OZl95rdunWrCQ0NdftdhYaGuu133759rfbdunUz5cuXt+aVL1/e7fnVrVu3TPsQFRVlpk+fbq0zJCTE+Pv7u603q/cLY4y1rdmzZ5uKFSsaSSYwMNCUKlXK7bnyzTffZFp2xYoVVpvseHq/a9y4sVvfZNzPS19LGffVk0WLFhmn02mtq3Tp0m4/R0REmD/++CPb+pcvX27tc+nSpY2fn581Lzw83Bw4cCDbfSyOCDfFwKXhZt26ddYL49I3hYIKN2XKlDFt27Y1v//+uzHm4gfM66+/boWcF154wYSEhJiePXuavXv3GmOMOXXqlHn++eetdSxdujTTtl0fyiEhIcbpdJpZs2aZs2fPGmOM2b9/v9uH2OLFizMt/5///Mf6kBo/frw5dOiQMcaYCxcumI0bN5rbbrvNSDJVqlQxp06d8tjPrjeh4cOHmyNHjhhjjElJSbH243IuXLhgbrrpJms/PvzwQ5OammqMMebPP/80d955pxUUvv7660zLX+7NNifyEm4OHjxowsLCjCRz9913m40bN5pz584ZY4w5fPiwiYmJsd6UP/vss0zL9+nTx8ydO9fs27fPmpaammp++OEH07RpUyPJNGrUKNNykydPtt7k09LSPNb2wQcfGEkmKCgo0+/twQcfNJJMtWrVzLx580xiYqIxxpizZ8+axYsXm2rVqhlJpmvXrm7LrV692gqRkydPNsePH7fmHTt2zHz33Xemb9++5u+//85ZB/5/uQ03xhir31u2bOk2PbvXbNu2ba0+XbdunUlPTzfGXOzzXbt2mVdffdVMmTIlx+vztA8BAQHG19fX9OvXz+zfv98Yc/H5vWfPHqttTsJNSEiIiYyMNN9//71VZ2xsrKlXr54VRuPj492WvZJwk5vls3u9bdq0yfj7+xtJpnnz5ubXX381xhiTlpZmvvjiC1OpUiUjyVSvXj3T8zLj9kNDQ03nzp3N9u3bjTEXf0eLFi2y/nDo3bt3tjUWR4SbYuDScGPMxb/CJJkGDRpYbxbGFFy4qVOnjklJScm0bO/eva02t99+u1ttLq4jMg8//HCmea4PZUnm3XffzTQ/LS3NtGzZ0qoho6SkJOsIz7fffutx386fP29uvPFGI8n861//cpvn6mdJZsiQIR6Xz4mFCxda6/nuu+881uAKP3Xr1s0031vh5qGHHjKSzAMPPJBlm2nTphlJpn79+rmq59SpU6ZChQpGklm9erXbvAMHDlhH2jz1lzHGtG/f3kgyDz74oNv0VatWGUkmLCzM+vC9VHx8vHXUZ8uWLdZ0V6hq3759rvblcvISbm6//XYjXTy6lVF2r9nAwEAjyaxduzbHteU23LiCbnZyEm5KlCjh8ejG4cOHTdmyZY0k8+STT7rNKwzhpmPHjkaSufbaa82ZM2cyzd+8ebMV+F955ZUst9+mTRuPwf3111830sWjWefPn8+2zuKGMTfF1IQJE+Tr66utW7dqwYIFBb79Z599NtM3OySpQ4cO1v9HjBjh8Xy5q012YywiIiLUv3//TNN9fHz0wgsvSJJ+//1365tF0sUxMidPnlTDhg3d6sjIz89P999/vyTpu+++89jGx8dHw4cPz7K2y1m0aJEkqVmzZmrfvr3HGsaMGSPp4jiVjPvgLSkpKZo/f74kZbvvffr0kST98ssvOnz4cI7XX6pUKbVq1UqStGbNGrd5lStX1m233SZJ+uCDDzIte+jQIesbOb1793ab9+6770qSevXqpYiICI/brlKlijX+IuPvvEyZMpKko0ePKi0tLcf7cjWULVtWknTixIkcL+Oq/9ChQ1ejJMvIkSOveB333nuvatWqlWl6WFiYHn/8cUn/97opLE6ePGk9X4YNG6aSJUtmatOwYUPdfffdkpTt+/CoUaPk45P547pLly6SLl7iY/fu3flRtm0QboqpmjVrWh/+MTExORqgm5+aNm3qcXqFChWs/zdp0iTbNv/73/+yXL/rInSe3HrrrfLzu3gVhI0bN1rTf/rpJ0nS9u3bVbFixSwfL730kqSLA3Y9ufbaaxUWFpZlbZfjqqldu3ZZtmnTpo18fX0z7YO3bNq0yRo42759+yz7LuN1djz135IlS9SzZ09Vq1ZNQUFBboM4P/roI0kXB8FeyhWaPvvss0wXtps/f77S0tIUHh6eqU9dv/N3330329/5Dz/8kKnmtm3bKiAgQFu2bNGtt96qd999V3FxcbnuO2+58847JUl9+/bV0KFD9eOPP1rftMsvgYGBatSo0RWvxxVes5t3/PjxQtX/mzdvljFGUvav5dtvv13SxT/WsnofvummmzxODw8Pt/6fm2BbHBBuirGxY8cqMDBQf/31l2bNmlWg2y5durTH6a7QkZM22QWyypUrZzkvICBA5cqVkyQdOXLEmu66cnNKSooOHz6c5cP17YasPgiuJNhkrOly+1C+fPlM++AtGa96nV3fZTxak7H/0tPT9cADD+iuu+7SRx99pLi4OJ07d06hoaHWRewCAgIkyeNVee+++26VKlVKZ86c0aeffuo2z3U0p1evXpn++nXVnZSUlG3NruCWsebq1avrP//5j0qVKqV169ZpwIABqlatmsLCwtSzZ08tXrzY+nC72lwfbK7ndU5MmTJFbdq00enTpzVt2jS1bt1awcHBaty4scaMGZMv38IpV66cxyMOuZXdayHjvMLwWnDJWEt29VepUkWSdOHChSwDSk7eLwv6D9TCjnBTjFWuXFlPPfWUJOnll1/W6dOnvVyRd7lOLfTs2VPm4ni0bB9ZfR3edUSlOMl4Wubs2bM56r+MXx1+9913tWDBAvn6+mr06NHavXu3UlNTdeLECevqyt27d5ckj4EhKCjIOrz//vvvW9N/++03/fLLL5Iyn5LKWPfMmTNzVPOlX1fu1auX9u3bp1mzZqlnz56KiIjQ0aNH9dFHH6lr165q1apVll/1zU+uU7S5uUpumTJltHz5cq1evVrPPfecmjdvLj8/P23atEkvvfSSatSoccWnrIvjawGFA+GmmBsxYoRCQ0N15MgRTZ06Ndu2Gf9KyO7aHYmJiflWX15l91dnamqqjh8/Lsn9KEvFihUlZX26qaC4avJ0+sUlJSXF4z54i6vvpLz138KFCyVJAwYM0Isvvqhrr70201/8CQkJ2a7DFV6WL19u/f5dR20aNGigevXqZVn3lfzOy5Ytq8cee0wLFy7U/v37tWfPHmu82OrVq6/6vYk2btxoHRHLy73AWrRoocmTJ2vNmjU6efKkFi9erHr16uns2bN66KGHcjU26mrJ7vWccV7G10JO3q+u5ntVxlqyey275vn5+Vljp3DlCDfFXGhoqEaMGCFJmjp1qo4ePZptW5f4+HiPbXbt2lUobqj3448/ZnlKYPXq1da9eBo3bmxNb968uaSL40eu9iDL7Lhqyu6y9CtXrrT2IauxSQWpSZMmKlGihCTpyy+/zPXyrudTw4YNPc4/ffq0YmNjs13HbbfdpipVqig9PV3z58+3/pX+b0zOpVy/c9e9uPJD9erVNXHiRD3wwAOSpKVLl+bbuj1x3fvI19dXDz744BWtKyAgQJ07d7ZO7aWkpLgN4M4YOAvqlJt08SKAl5tXtmxZt4sM5uT9Krvn1JXua6NGjax1ZPdado3nql+/fr7c/wsXEW6gp556SlWqVNGpU6c0bty4LNsFBQVZh70/+eQTj23Gjx9/VWrMrf379+u9997LND09PV0TJkyQJNWuXdvtr/l7771XZcqU0fnz5zVkyJBs39DS09OvWoi77777JEnr1q3T999/n2n+hQsXrEHNdevWVd26da9KHbkRFBRkfZhPnjxZ+/fvz7b9pWMLXDdNdJ1CutS4ceN06tSpbNfp4+OjXr16Sbp4xMZ1BMfX19eq7VKPPvqopIvfOps5c2a26z9z5ozblYpTU1OzbR8YGGjVdTWkpaVp6NChVjB75JFHdO211+Zo2QsXLig9PT3L+a7aJff6g4ODrf8X5B8xH3/8sXbu3Jlp+rFjx/TWW29Jung6OaPrrrvO2g9P71fp6enWFds9udJ9LVOmjPWty1deecXjGL1ffvnFqs31LUzkD8INFBgYaB06v9xf3a4X4OzZs/Xmm2/q7Nmzki7+ZTRgwAAtWrTI41ceC1pISIieeOIJvfPOO9Yh6fj4eN1///3WX3ovv/yy2zJlypTR9OnTJV08TdKpUyfFxsZaHwLp6enavn27pk6dqjp16uTrX/sZ3XPPPda3I3r06KH58+dbgwXj4uJ0zz33WLcxmDJlylWpweX8+fM6duxYtg/XAN8JEyYoPDxcx44dU7NmzfTBBx+4BZKjR4/qk08+Ubdu3TK9kXfs2FGS9M477+jtt9+2QkRCQoKeffZZTZkyJUeDZV2npn777TfrK8jt27d3+xZeRq1atbK+NThw4EA9++yz+uuvv6z5qampWr9+vZ577jlFRUW5DRIdNGiQevTooU8++cRt+unTpzVr1ixr7I/r9gj5wRij3bt36+2331ajRo00bdo0SRe/ufXaa6/leD0HDhywbmWyZcsWt7uK//rrr9YRoKCgIOsr+NLF14hrcOycOXOyvBt5fgsICFDHjh31ww8/WH90bNiwQe3atdOxY8dUunRp6wi0i7+/v+655x5JF5+bH330kfW82rlzp7p165bt5SSuu+4662jkf/7znzwdvXn55Zfl7++vPXv2qEOHDtZlG9LT0/X111/rH//4hy5cuKDq1avrsccey/X6kY2rehUdFAqeLuJ3qQsXLpiaNWtaF42Sh4taGXPxYmq1a9e22vj4+FgXvvP39zcLFizI0UX8sroAWE4unJXdRbMy3n6hRYsWVl0ZL6Wu/38F5KzMnDnT7XYLTqfTlCtXzrrSqOvx4Ycfui2X8fYLV+rAgQPWbQX0/y9ilvEWEj4+Pua1117zuGx+XsQvJ4/Bgwdby/3xxx/muuuuc6uzbNmy1kXwXI927dq5be9///uf2/PP9bxyXbb/scceM3379jWS+y0BPGnUqJHbthYsWJBt+9TUVDNgwAC3ZUqVKmVCQ0PdbsMhye0y9656Mi5z6W0+WrRoYU6fPp2rvs94AbyMtzgoW7as221KpIuX4n/55ZezvIBbVq+5jNOli7deKFu2rNvzvkSJEubjjz/OtM5x48a5vTYiIiJMVFSU2y1RcvMczO3tF0qWLOl2+wWn02mWLFnicd3x8fEmPDzcauvv72/dnqR06dJm5cqV2b7fPfzww9b8kiVLmsjISBMVFWWGDh2a431duHChW78GBwebgIAA6+ec3H4hO9nVX5xx5AaSLp6vd52uyU6pUqW0Zs0aDRkyRNHR0fLz87P+Qlq3bp11SsXbSpQooWXLlmnChAm6/vrrlZqaqpCQELVt21ZfffVVtqffHn/8ce3cuVP//Oc/Vb9+fTmdTp08eVKlSpVS48aN9dRTT2np0qVX9TBy5cqVtXHjRk2bNk0333yzAgMDlZycrIiICPXu3VubNm0qlHeBrlWrln799Ve99dZbat++vcqXL6+kpCQZY3Tttdfq3nvv1dtvv21ds8alTJkyWrt2rZ555hlVrVpVvr6+8vPzU+vWrbVgwYJcXaog4/ia4OBg60JnWSlRooTeeecdrV27Vv369VP16tWVlpam06dPKywsTK1bt9bo0aP166+/un2lNyYmRq+//rq6deummjVrys/Pz1rm9ttv1+zZs7Vy5UoFBQXluPZLHTt2zO3r6Ndcc41uvPFGPfzww3rvvfd06NAhPf/8826DZ3OicuXK+uKLL/Tss8/q5ptvVqVKlXT69Gn5+fmpdu3aGjhwoLZt22Z9Qy2jUaNG6bXXXlPjxo3l7++vAwcOaN++fZcd8H0loqOjtWXLFg0cOFDXXHONzp07p7CwMN1///3asmVLlkfHqlSpotjYWA0YMMD63ZUqVUp9+vTR5s2b3Y5KeTJjxgyNHTvWOn29f/9+7du3L8ub53rSs2dP/f7773rsscdUvXp1paamys/PTw0aNNCLL76obdu2ebxAIa6Mw5gCHBUGAABwlXHkBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2EruLo5gE+np6Tp48KBKly4th8Ph7XIAAEAOGGN06tQphYeHZ3trk2IZbg4ePKiIiAhvlwEAAPIgPj5eVapUyXJ+sQw3pUuXlnSxczLeHA0AABReSUlJioiIsD7Hs1Isw43rVFRwcDDhBgCAIuZyQ0oYUAwAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGzFz9sF2M3+/ft17Ngxb5eRa6mpqXI6nd4uI1eouWBQc8Gg5oJBzQWjfPnyioyM9Nr2CTf5aP/+/ap1fS0lpyR7u5Rc85GP0pXu7TJyhZoLBjUXDGouGNRcMEoGlNT2ndu9FnAIN/no2LFjSk5J1iiNUpSivF1OjsUqVrM1u0jVTc0Fg5oLBjUXDGouGPu0TxNSJujYsWOEGzuJUpSu03XeLiPH9mu/pKJVNzUXDGouGNRcMKi5+GBAMQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsBXCDQAAsJVCF25WrVqlu+66S+Hh4XI4HPr888+teefPn9fw4cNVr149BQUFKTw8XH369NHBgwe9VzAAAChUCl24OXPmjOrXr68ZM2ZkmpecnKzNmzcrJiZGmzdv1qeffqqdO3eqc+fOXqgUAAAURn7eLuBSd9xxh+644w6P80JCQrR06VK3aW+88YaaNm2q/fv3KzIysiBKBAAAhVihCze5lZiYKIfDoTJlymTZJjU1VampqdbPSUlJBVAZAADwhkJ3Wio3UlJSNHz4cN1///0KDg7Ost3EiRMVEhJiPSIiIgqwSgAAUJCKbLg5f/68evToIWOMZs6cmW3bkSNHKjEx0XrEx8cXUJUAAKCgFcnTUq5gs2/fPi1fvjzbozaS5HQ65XQ6C6g6AADgTUUu3LiCze7du7VixQqVK1fO2yUBAIBCpNCFm9OnT2vPnj3Wz3Fxcdq6davKli2rSpUqqXv37tq8ebOWLFmitLQ0JSQkSJLKli2rEiVKeKtsAABQSBS6cLNx40a1adPG+nnIkCGSpL59+2rs2LH64osvJEkNGjRwW27FihVq3bp1QZUJAAAKqUIXblq3bi1jTJbzs5sHAABQZL8tBQAA4AnhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2ArhBgAA2EqhCzerVq3SXXfdpfDwcDkcDn3++edu840xGj16tCpVqqTAwEC1a9dOu3fv9k6xAACg0Cl04ebMmTOqX7++ZsyY4XH+lClT9Prrr2vWrFmKjY1VUFCQOnTooJSUlAKuFAAAFEZ+3i7gUnfccYfuuOMOj/OMMZo+fbpeeOEFdenSRZL0/vvvq0KFCvr888913333FWSpAACgECp0R26yExcXp4SEBLVr186aFhISoptuuknr1q3LcrnU1FQlJSW5PQAAgD0VqXCTkJAgSapQoYLb9AoVKljzPJk4caJCQkKsR0RExFWtEwAAeE+RCjd5NXLkSCUmJlqP+Ph4b5cEAACukiIVbipWrChJOnz4sNv0w4cPW/M8cTqdCg4OdnsAAAB7KlLhJjo6WhUrVtSyZcusaUlJSYqNjVWzZs28WBkAACgsCt23pU6fPq09e/ZYP8fFxWnr1q0qW7asIiMj9cwzz+jll19WjRo1FB0drZiYGIWHh6tr167eKxoAABQahS7cbNy4UW3atLF+HjJkiCSpb9++mjt3rp577jmdOXNGjz76qE6ePKkWLVro22+/VUBAgLdKBgAAhUihCzetW7eWMSbL+Q6HQy+99JJeeumlAqwKAAAUFUVqzA0AAMDlEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtEG4AAICtFLlwk5aWppiYGEVHRyswMFDVq1fXuHHjZIzxdmkAAKAQ8PN2Abk1efJkzZw5U++9957q1KmjjRs3qn///goJCdHTTz/t7fIAAICXFblws3btWnXp0kWdOnWSJFWtWlULFizQzz//7OXKAABAYVDkTkvdcsstWrZsmXbt2iVJ+uWXX7RmzRrdcccdWS6TmpqqpKQktwcAALCnInfkZsSIEUpKSlLNmjXl6+urtLQ0jR8/Xr169cpymYkTJ+rFF18swCoBAIC3FLkjNx999JHmzZun+fPna/PmzXrvvff06quv6r333stymZEjRyoxMdF6xMfHF2DFAACgIBW5IzfDhg3TiBEjdN9990mS6tWrp3379mnixInq27evx2WcTqecTmdBlgkAALykyB25SU5Olo+Pe9m+vr5KT0/3UkUAAKAwKXJHbu666y6NHz9ekZGRqlOnjrZs2aJp06bpoYce8nZpAACgEChy4ebf//63YmJi9OSTT+rIkSMKDw/XY489ptGjR3u7NAAAUAjkOdysWrVKVatWVWRkZJZt4uPjFRcXp5YtW+Z1M5mULl1a06dP1/Tp0/NtnQAAwD7yPOamTZs2mjt3brZt3n//fbVp0yavmwAAAMi1PIebnNzLKT09XQ6HI6+bAAAAyLWr+m2p3bt3KyQk5GpuAgAAwE2uxtxc+o2kzz//XHv37s3ULi0tTfHx8Vq1alW2t0UAAADIb7kKNxnH2DgcDm3dulVbt2712NbhcKhJkyb617/+dSX1AQAA5Equwk1cXJyki+NtqlWrpmeeeUaDBw/O1M7X11ehoaEKCgrKnyoBAAByKFfhJioqyvr/nDlz1LBhQ7dpAAAA3pbn69xkdR8nAAAAb7riKxT//PPP2rBhg06ePKm0tLRM8x0Oh2JiYq50MwAAADmS53Bz4sQJde3aVT/99FO217wh3AAAgIKU53AzZMgQrVmzRq1bt1bfvn1VpUoV+fkVuVtVAQAAm8lzGlmyZImaNm2qZcuWcRViAABQaOT5CsVnz55Vy5YtCTYAAKBQyXO4adCggcerEwMAAHhTnsPNmDFj9MUXX2j9+vX5WQ8AAMAVyfOYm4SEBHXq1EmtWrVSr1691KhRIwUHB3ts26dPnzwXCAAAkBt5Djf9+vWTw+GQMUZz587V3LlzM42/McbI4XAQbgAAQIHJc7iZM2dOftYBAACQL7j9AgAAsJU8DygGAAAojPJ85Gb//v05bhsZGZnXzQAAAORKnsNN1apVc3QBP4fDoQsXLuR1MwAAALmS53DTp08fj+EmMTFRv/zyi+Li4tSqVStVrVr1SuoDAADIlTyHm7lz52Y5zxijqVOnasqUKXr33XfzugkAAIBcuyoDih0Oh/75z3+qTp06GjZs2NXYBAAAgEdX9dtSjRs31vLly6/mJgAAANxc1XDz559/MpgYAAAUqDyPuclKenq6/v77b82dO1eLFy9W27Zt83sTAAAAWcpzuPHx8cn2q+DGGIWGhmrq1Kl53QQAAECu5TnctGzZ0mO48fHxUWhoqJo0aaL+/fsrLCzsigoEAADIjTyHm5UrV+ZjGQAAAPmDe0sBAABbyZcBxT/99JO2bt2qpKQkBQcHq0GDBmrevHl+rBoAACBXrijcrF27Vv3799eePXskXRxE7BqHU6NGDc2ZM0fNmjW78ioBAAByKM/h5vfff1f79u2VnJys22+/XW3atFGlSpWUkJCgFStW6Pvvv1eHDh20fv161a5dOz9rBgAAyFKew81LL72kc+fO6euvv1bHjh3d5g0fPlzffvutOnfurJdeekkLFy684kIBAAByIs8DileuXKnu3btnCjYuHTt2VPfu3bVixYo8FwcAAJBbeQ43iYmJio6OzrZNdHS0EhMT87oJAACAXMtzuAkPD9f69euzbRMbG6vw8PC8bgIAACDX8hxuOnfurJUrVyomJkYpKSlu81JSUjRmzBitWLFCXbp0ueIiAQAAcirPA4pjYmK0ZMkSTZgwQW+99ZaaNm2qChUq6PDhw9qwYYOOHj2qatWqKSYmJj/rBQAAyFaew025cuW0fv16Pffcc1q4cKG+/vpra15AQID69++vyZMnq2zZsvlSKAAAQE5c0UX8ypcvr9mzZ+utt97Sjh07rCsU16xZU/7+/vlVIwAAQI7leszN+PHjNWrUKJ0/f96a5u/vr3r16ql58+aqV6+ejDF6/vnnNWnSpHwt1uXvv//Wgw8+qHLlyikwMFD16tXTxo0br8q2AABA0ZKrcPPDDz9o9OjRKleuXLZHZkqUKKFy5crp+eefz/fr3Pzvf/9T8+bN5e/vr2+++UZ//PGHpk6dqtDQ0HzdDgAAKJpyFW7ef/99hYaGatCgQZdtO3DgQJUtW1Zz5szJc3GeTJ48WREREZozZ46aNm2q6OhotW/fXtWrV8/X7QAAgKIpV+Fm7dq1ateunZxO52XbOp1OtWvXTj/99FOei/Pkiy++UOPGjXXvvfcqLCxMDRs21DvvvJPtMqmpqUpKSnJ7AAAAe8pVuDl48KCqVauW4/bR0dE6dOhQrovKzl9//aWZM2eqRo0a+u677/TEE0/o6aef1nvvvZflMhMnTlRISIj1iIiIyNeaAABA4ZGrcOPj4+M2kPhyzp8/Lx+fPF8n0KP09HQ1atRIEyZMUMOGDfXoo4/qkUce0axZs7JcZuTIkUpMTLQe8fHx+VoTAAAoPHKVPMLDw7Vt27Yct9+2bZsqV66c66KyU6lSJdWuXdttWq1atbR///4sl3E6nQoODnZ7AAAAe8pVuLn11lu1fPly7d2797Jt9+7dq+XLl6tly5Z5rc2j5s2ba+fOnW7Tdu3apaioqHzdDgAAKJpyFW4GDhyo8+fPq3v37jp27FiW7Y4fP657771XFy5c0BNPPHHFRWb07LPPav369ZowYYL27Nmj+fPn6+2339bAgQPzdTsAAKBoytUVihs1aqRnnnlG06dPV+3atfX444+rTZs2qlKliqSLF9dbtmyZ3n77bR09elRDhgxRo0aN8rXgJk2a6LPPPtPIkSP10ksvKTo6WtOnT1evXr3ydTsAAKBoyvXtF6ZOnaqAgAC98sorGj9+vMaPH+823xgjX19fjRw5Ui+//HK+FZrRnXfeqTvvvPOqrBsAABRtuQ43DodDEyZM0MMPP6w5c+Zo7dq1SkhIkCRVrFhRzZs3V79+/bioHgAA8Io83zizevXqV+3IDAAAQF7l70VoAAAAvIxwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbIVwAwAAbKXIh5tJkybJ4XDomWee8XYpAACgECjS4WbDhg166623dMMNN3i7FAAAUEgU2XBz+vRp9erVS++8845CQ0O9XQ4AACgkimy4GThwoDp16qR27dpdtm1qaqqSkpLcHgAAwJ78vF1AXixcuFCbN2/Whg0bctR+4sSJevHFF69yVQAAoDAockdu4uPjNXjwYM2bN08BAQE5WmbkyJFKTEy0HvHx8Ve5SgAA4C1F7sjNpk2bdOTIETVq1MialpaWplWrVumNN95QamqqfH193ZZxOp1yOp0FXSoAAPCCIhdu2rZtq99++81tWv/+/VWzZk0NHz48U7ABAADFS5ELN6VLl1bdunXdpgUFBalcuXKZpgMAgOKnyI25AQAAyE6RO3LjycqVK71dAgAAKCQ4cgMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGylyIWbiRMnqkmTJipdurTCwsLUtWtX7dy509tlAQCAQqLIhZsff/xRAwcO1Pr167V06VKdP39e7du315kzZ7xdGgAAKAT8vF1Abn377bduP8+dO1dhYWHatGmTWrZs6aWqAABAYVHkjtxcKjExUZJUtmxZL1cCAAAKgyJ35Caj9PR0PfPMM2revLnq1q2bZbvU1FSlpqZaPyclJRVEeQAAwAuK9JGbgQMHatu2bVq4cGG27SZOnKiQkBDrERERUUAVAgCAglZkw82gQYO0ZMkSrVixQlWqVMm27ciRI5WYmGg94uPjC6hKAABQ0IrcaSljjJ566il99tlnWrlypaKjoy+7jNPplNPpLIDqAACAtxW5cDNw4EDNnz9fixcvVunSpZWQkCBJCgkJUWBgoJerAwAA3lbkTkvNnDlTiYmJat26tSpVqmQ9Fi1a5O3SAABAIVDkjtwYY7xdAgAAKMSK3JEbAACA7BBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRBuAACArRTZcDNjxgxVrVpVAQEBuummm/Tzzz97uyQAAFAIFMlws2jRIg0ZMkRjxozR5s2bVb9+fXXo0EFHjhzxdmkAAMDLimS4mTZtmh555BH1799ftWvX1qxZs1SyZEnNnj3b26UBAAAvK3Lh5ty5c9q0aZPatWtnTfPx8VG7du20bt06L1YGAAAKAz9vF5Bbx44dU1pamipUqOA2vUKFCtqxY4fHZVJTU5Wammr9nJiYKElKSkrK19pOnz4tSdqlXTqrs/m67qtpn/ZJKlp1U3PBoOaCQc0Fg5oLRrziJV38TMzvz1nX+owx2Tc0Rczff/9tJJm1a9e6TR82bJhp2rSpx2XGjBljJPHgwYMHDx48bPCIj4/PNisUuSM35cuXl6+vrw4fPuw2/fDhw6pYsaLHZUaOHKkhQ4ZYP6enp+vEiRMqV66cHA5HvtWWlJSkiIgIxcfHKzg4ON/WW9TRL57RL57RL57RL57RL57ZtV+MMTp16pTCw8OzbVfkwk2JEiV04403atmyZeratauki2Fl2bJlGjRokMdlnE6nnE6n27QyZcpctRqDg4Nt9WTKL/SLZ/SLZ/SLZ/SLZ/SLZ3bsl5CQkMu2KXLhRpKGDBmivn37qnHjxmratKmmT5+uM2fOqH///t4uDQAAeFmRDDc9e/bU0aNHNXr0aCUkJKhBgwb69ttvMw0yBgAAxU+RDDeSNGjQoCxPQ3mL0+nUmDFjMp0CK+7oF8/oF8/oF8/oF8/oF8+Ke784jLnc96kAAACKjiJ3ET8AAIDsEG4AAICtEG4AAICtEG4AAICtEG7yYOLEiWrSpIlKly6tsLAwde3aVTt37nRrk5KSooEDB6pcuXIqVaqU7rnnnkxXVbazSZMmyeFw6JlnnrGmFdc++fvvv/Xggw+qXLlyCgwMVL169bRx40ZrvjFGo0ePVqVKlRQYGKh27dpp9+7dXqz46ktLS1NMTIyio6MVGBio6tWra9y4cW73iykO/bJq1SrdddddCg8Pl8Ph0Oeff+42Pyd9cOLECfXq1UvBwcEqU6aMHn74Yes+d0VVdv1y/vx5DR8+XPXq1VNQUJDCw8PVp08fHTx40G0dxa1fLvX444/L4XBo+vTpbtPt2C+eEG7y4Mcff9TAgQO1fv16LV26VOfPn1f79u115swZq82zzz6rL7/8Uh9//LF+/PFHHTx4UHfffbcXqy44GzZs0FtvvaUbbrjBbXpx7JP//e9/at68ufz9/fXNN9/ojz/+0NSpUxUaGmq1mTJlil5//XXNmjVLsbGxCgoKUocOHZSSkuLFyq+uyZMna+bMmXrjjTe0fft2TZ48WVOmTNG///1vq01x6JczZ86ofv36mjFjhsf5OemDXr166ffff9fSpUu1ZMkSrVq1So8++mhB7cJVkV2/JCcna/PmzYqJidHmzZv16aefaufOnercubNbu+LWLxl99tlnWr9+vcdbFNixXzy68ltZ4siRI0aS+fHHH40xxpw8edL4+/ubjz/+2Gqzfft2I8msW7fOW2UWiFOnTpkaNWqYpUuXmlatWpnBgwcbY4pvnwwfPty0aNEiy/np6emmYsWK5pVXXrGmnTx50jidTrNgwYKCKNErOnXqZB566CG3aXfffbfp1auXMaZ49osk89lnn1k/56QP/vjjDyPJbNiwwWrzzTffGIfDYf7+++8Cq/1qurRfPPn555+NJLNv3z5jTPHulwMHDpjKlSubbdu2maioKPOvf/3Lmlcc+sWFIzf5IDExUZJUtmxZSdKmTZt0/vx5tWvXzmpTs2ZNRUZGat26dV6psaAMHDhQnTp1ctt3qfj2yRdffKHGjRvr3nvvVVhYmBo2bKh33nnHmh8XF6eEhAS3fgkJCdFNN91k63655ZZbtGzZMu3atUuS9Msvv2jNmjW64447JBXffskoJ32wbt06lSlTRo0bN7batGvXTj4+PoqNjS3wmr0lMTFRDofDumdgce2X9PR09e7dW8OGDVOdOnUyzS9O/VJkr1BcWKSnp+uZZ55R8+bNVbduXUlSQkKCSpQokenmnBUqVFBCQoIXqiwYCxcu1ObNm7Vhw4ZM84prn/z111+aOXOmhgwZolGjRmnDhg16+umnVaJECfXt29fa90tvHWL3fhkxYoSSkpJUs2ZN+fr6Ki0tTePHj1evXr0kqdj2S0Y56YOEhASFhYW5zffz81PZsmWLTT+lpKRo+PDhuv/++60bRBbXfpk8ebL8/Pz09NNPe5xfnPqFcHOFBg4cqG3btmnNmjXeLsWr4uPjNXjwYC1dulQBAQHeLqfQSE9PV+PGjTVhwgRJUsOGDbVt2zbNmjVLffv29XJ13vPRRx9p3rx5mj9/vurUqaOtW7fqmWeeUXh4eLHuF+TO+fPn1aNHDxljNHPmTG+X41WbNm3Sa6+9ps2bN8vhcHi7HK/jtNQVGDRokJYsWaIVK1aoSpUq1vSKFSvq3LlzOnnypFv7w4cPq2LFigVcZcHYtGmTjhw5okaNGsnPz09+fn768ccf9frrr8vPz08VKlQodn0iSZUqVVLt2rXdptWqVUv79++XJGvfL/3WmN37ZdiwYRoxYoTuu+8+1atXT71799azzz6riRMnSiq+/ZJRTvqgYsWKOnLkiNv8Cxcu6MSJE7bvJ1ew2bdvn5YuXWodtZGKZ7+sXr1aR44cUWRkpPUevG/fPg0dOlRVq1aVVLz6hXCTB8YYDRo0SJ999pmWL1+u6Ohot/k33nij/P39tWzZMmvazp07tX//fjVr1qygyy0Qbdu21W+//aatW7daj8aNG6tXr17W/4tbn0hS8+bNM10mYNeuXYqKipIkRUdHq2LFim79kpSUpNjYWFv3S3Jysnx83N9+fH19lZ6eLqn49ktGOemDZs2a6eTJk9q0aZPVZvny5UpPT9dNN91U4DUXFFew2b17t3744QeVK1fObX5x7JfevXvr119/dXsPDg8P17Bhw/Tdd99JKmb94u0RzUXRE088YUJCQszKlSvNoUOHrEdycrLV5vHHHzeRkZFm+fLlZuPGjaZZs2amWbNmXqy64GX8tpQxxbNPfv75Z+Pn52fGjx9vdu/ebebNm2dKlixpPvzwQ6vNpEmTTJkyZczixYvNr7/+arp06WKio6PN2bNnvVj51dW3b19TuXJls2TJEhMXF2c+/fRTU758efPcc89ZbYpDv5w6dcps2bLFbNmyxUgy06ZNM1u2bLG+9ZOTPujYsaNp2LChiY2NNWvWrDE1atQw999/v7d2KV9k1y/nzp0znTt3NlWqVDFbt251ew9OTU211lHc+sWTS78tZYw9+8UTwk0eSPL4mDNnjtXm7Nmz5sknnzShoaGmZMmSplu3bubQoUPeK9oLLg03xbVPvvzyS1O3bl3jdDpNzZo1zdtvv+02Pz093cTExJgKFSoYp9Np2rZta3bu3OmlagtGUlKSGTx4sImMjDQBAQGmWrVq5vnnn3f7cCoO/bJixQqP7yV9+/Y1xuSsD44fP27uv/9+U6pUKRMcHGz69+9vTp065YW9yT/Z9UtcXFyW78ErVqyw1lHc+sUTT+HGjv3iicOYDJcEBQAAKOIYcwMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMAAGyFcAMUYq1bt+YmePmgatWq1v117OD1119XnTp1VLJkSTkcDk2fPt3bJQGFCuEGRd7evXvlcDjUsWNHb5die3PnzpXD4ZDD4bBucnmpSZMmyeFwaO7cuQVbXDGxcOFCDR48WE6nU4MHD9aYMWN08803Z7uMKyQnJCQUUJWAd/l5uwAAWXv//feVnJzs7TI8mjx5sh577DGVLVvW26UUK0uWLLH+DQ8P93I1QOHEkRugEIuMjFTNmjW9XUYm1atXV2JiosaPH+/tUoqdgwcPShLBBsgG4QbFzqlTpzRmzBjVqVNHgYGBKlOmjDp06KA1a9Zkartp0yYNGjRIdevWVUhIiAIDA1WvXj1NmjRJ58+fz9TeNbbj5MmTGjRokCIiIuTn56e5c+dap8/69eunPXv2qFu3bgoNDVVQUJDatWunX375JdP6PI25cZ0amjt3rr7//nvdcsstKlmypMqVK6e+ffvq+PHjHvf7rbfeUp06dRQQEKCIiAg999xzSklJkcPhUOvWrXPVh/369dO1116rGTNmaP/+/Zdtn3HfPfFUg2vfU1NTNWrUKEVGRiowMFA33nijfvjhB0lSYmKiBg4cqPDwcAUEBKhZs2b6+eefs6zj5MmTeuyxx1SxYkUFBASoYcOGWrBggce2xhjNnj1bzZs3V3BwsEqWLKnGjRtr9uzZmdqOHTtWDodDK1eu1Ny5c9WoUSOVLFkyx/365Zdfqk2bNtZzrH79+po2bZouXLhgtXH93lesWGH1meuR3xYvXqy2bdsqNDRUAQEBqlu3rl599VWlpaW5tUtMTNTkyZPVqlUrhYeHq0SJEgoPD1efPn30559/urUdN26cHA6H3n//fY/b/PTTT+VwOPT888+7TY+Li9OAAQMUGRkpp9OpSpUqqV+/ftq3b1+mdWzevFndu3e32l5zzTVq0qQJIbwY4rQUipUTJ06oZcuW+v3339W8eXM9/vjjSkpK0uLFi9WmTRt9/PHH6tq1q9X+nXfe0ZdffqmWLVvqH//4h5KTk7Vy5UqNHDlSGzZs0CeffJJpG6mpqbrtttt0+vRpde7cWX5+fqpQoYI1f+/evbr55ptVp04dPfTQQ/rzzz+t7W/fvt2tbXa++OILffXVV7rrrrt0yy23aNWqVXr//ff1559/Zgpqo0eP1rhx41ShQgU98sgj8vf310cffaQdO3bkqR/9/Pw0fvx49ezZUzExMXrvvffytJ6c6Nmzp3777Td17txZZ8+e1bx583TnnXfqp59+0qOPPqpz587p3nvv1dGjR7Vo0SJ17NhRcXFxCgkJcVvPuXPn1K5dO50+fVq9e/fWmTNn9NFHH+mBBx7QsWPH9NRTT1ltjTHq1auXFixYoBo1auiBBx5QiRIltHTpUj388MP6448/9Oqrr2aq9ZVXXtGKFSvUpUsXtW/fXr6+vpfdv2nTpmno0KEqW7asHnjgAQUFBemLL77Q0KFDtXr1autDv0GDBhozZozmzp2rffv2acyYMVfeuR6MHDlSkyZNUuXKlXX33XcrJCREq1ev1rBhwxQbG6uPP/7Yart9+3aNHj1abdq0Ubdu3RQUFKQdO3Zo/vz5+uqrr7R582ZFRUVJkh588EGNGTNGH374ofr06ZNpux988IEkqXfv3ta02NhYdejQQWfOnNGdd96pGjVqaO/evZo3b56++eYbrVu3TtWqVZMkbd26Vbfccot8fX3VpUsXRUVF6eTJk/rjjz/09ttvZwpNsDnv3pQcuHJxcXFGkunQocNl2z7wwANGknnnnXfcph8+fNhERESYa665xpw9e9aavm/fPnPhwgW3tunp6eahhx4yksyaNWvc5kVFRVm1JCcne6xTkpk0aZLbvBdeeMFIMhMnTnSb3qpVK3Ppy3TOnDlGkvHz83Pb/oULF0zr1q2NJLNu3Tpr+s6dO42vr6+pXLmyOXz4sDU9KSnJ1K5d20gyrVq1yqrLPG574sSJJj093TRp0sT4+PiYX375xWozceJEI8nMmTMn07737dvX43o91eDa9xYtWpjTp09b0xctWmQkmTJlyph7773XnD9/3po3efJkI8lMnTrVbV2u30vLli1NamqqNT0+Pt6UL1/eOJ1Oc+DAAWv622+/bSSZ/v37m3PnzlnTU1NTzV133WUkmY0bN1rTx4wZYySZoKAg8+uvv2bfiRns2bPH+Pn5mbCwMLN//35rekpKimnRooWRZN5//32P/ZIbrmUOHTqUbbvvv//eev5m7PP09HTz+OOPG0nmv//9rzX95MmT5vjx45nWs3z5cuPj42MGDBjgNr1FixbG19fXHDx40G368ePHTYkSJUzjxo2taefOnTNVq1Y1pUuXNps3b3Zrv3r1auPr62vuvPNOa9qQIUOMJPP5559nqufYsWPZ7jfsh9NSKDaOHTumRYsW6bbbbtOAAQPc5oWFhWnYsGE6evSodcpDujjm5dK/vh0OhwYOHChJbm0zmjJligIDAz3Oi46O1rBhw9ymPfzww5KkDRs25Hh/HnjgATVv3tz62dfXV3379s20ngULFigtLU1Dhw5VWFiYNb106dJ64YUXcry9SzkcDk2ePFnp6ekaMWJEntdzOePHj1dQUJD1c/fu3eXv76+TJ0/q1VdflZ/f/x2Avv/++yXJ4yk+SZowYYJKlChh/VylShUNHjxYqampWrhwoTX9jTfeUFBQkGbMmCF/f39reokSJaxTHJ5OZz366KOqV69ejvdt/vz5unDhgoYOHaqIiAhrutPp1OTJkyWpQL919sYbb0iS3n77bbc+dzgc1rfgMu53SEiIxwHlbdq0UZ06dTK9Pnr37q20tLRMfbdo0SKdO3dODz74oDVtyZIl2rt3r4YNG6aGDRu6tW/RooW6dOmir7/+WklJSW7zPL3uypUrd7ldh81wWgrFxoYNG5SWlqbU1FSNHTs20/zdu3dLknbs2KE777xT0sVTGW+88YYWLlyoHTt26PTp0zLGWMu4BndmFBAQkO0HXIMGDeTj4/53RZUqVSRdHBOSUzfeeGOmaZ7W4/qgb9GiRab2GcNRXrRp00YdO3bUN998ox9//FGtWrW6ovV50qBBA7effXx8FBYWpuTkZEVGRrrNq1SpkiTPvxc/Pz81a9Ys0/Rbb71VkrRlyxZJUnJysn777TeFh4dbASMj11grT6f0mjZtmoM9+j+ubXoam9OsWTMFBARo69atuVrnlVi/fr2CgoI8jiuSLgaHS/d75cqVmj59umJjY3Xs2DG3cUIZg6Qk9ejRQ08//bQ++OADDRkyxJr+4Ycfys/PzwqnrlokaefOnR5frwkJCUpPT9euXbvUuHFj9ejRQ9OnT1e3bt3Us2dP3X777WrZsqUqV66c635A0Ue4QbFx4sQJSdJPP/2kn376Kct2Z86csf7fvXt3ffnll7ruuuvUs2dPhYWFWUcNXnvtNaWmpmZaPiwsLNtBnsHBwZmmuY4+XDpgMzs5XY/rL9uMR21ccjq+JzuTJk3S999/r+eee06xsbFXvL5LZbWf2e2/p8He5cuXzxQqpf/rg8TEREnS//73Pxlj9Pfff+vFF1/Msq6Mz5NL15VTrt+Np+UcDocqVKigv//+O1frvBInTpzQhQsXcrzfH3/8sXr27KlSpUqpQ4cOqlq1qnVhQdfYoIzKlCmjO++8U5988on++OMP1a5dW3/++afWrl2rf/zjH27PUdfrdd68ednW7Krnpptu0sqVKzVhwgTNnz9fc+bMkSQ1adJEkydPVps2bXLXGSjSCDcoNlwfhkOHDvU4GPRSGzZs0JdffqkOHTroq6++cjs9tX79er322mselytsVxR27feRI0eswZ0uhw8fvuL1169fX7169dIHH3zgNtg0I1eoyPhXvYsrVFxtx44dU3p6eqaA4+oD1wBkV3/deOON2rhxY662kdvfvWtbhw8fzvS7Mcbo8OHDHkPc1RIcHCyHw6Fjx47lqP3YsWMVEBCgTZs2qUaNGm7zMp7my6h379765JNP9MEHH2jixIn68MMPremX1iJd/CaZ60jq5dx666365ptvdPbsWcXGxurLL7/Um2++qU6dOmnbtm3W4GPYH2NuUGw0adJEDodD69aty1F711dZO3XqlGnczerVq/O9vqulfv36kuTxaNXatWvzZRvjxo2T0+nU888/7zHAlClTRpI8HoVwnZq52i5cuODxd+/6XbrGdZQuXVq1atXS9u3bc3WaMC9c21y5cmWmebGxsUpJScl0Wu5quummm3T8+HHrFO3l/Pnnn6pVq1amYHPo0CH99ddfHpf5xz/+oXLlymn+/PlKT0/XvHnzVLp0aXXp0iVTLZJy/HrNKDAwUK1bt9bUqVM1atQonT17VkuXLs31elB0EW5QbFSsWFE9evTQ2rVr9corr7iNnXGJjY21rgjs+kv60q9V//7771neeqAwuu++++Tj46OpU6e6/UV+5syZfLv+R1RUlJ588knt3r3b4wDY4OBgXX/99VqzZo327NljTT916pRGjhyZLzXkxKhRo3Tu3Dnr5wMHDui1116T0+nUfffdZ01/+umnlZycrEceecTj6ae4uDjt3bv3iut54IEH5Ofnp2nTprmNEzp37pyGDx8uSVleG+hqePrppyVJDz30kMfrJSUkJGj79u3Wz1FRUdqzZ4/bEcCUlBQ98cQTHk8NSpK/v7969uyp/fv3a8qUKdq9e7fuueeeTAOBu3TposjISE2bNk2rVq3KtJ7z58+7vTbXrVunlJSUTO1ctQUEBGS367AZTkvBNn777bcsPwhq1qypESNG6M0339TOnTv13HPP6YMPPlCzZs1UpkwZxcfHa+PGjdq9e7cOHTqkkiVLqmnTpmratKk++ugjHTp0SDfffLP279+vL774Qp06ddJ///vfgt3BPLr++us1YsQITZgwQfXq1VOPHj3k5+enTz/9VPXq1dO2bds8jkXJreeff16zZ8/OdPE2l6FDh+rRRx9Vs2bNdO+99yo9PV3ffPONmjRpcsXbzolKlSrpzJkzuuGGG3TXXXdZ17k5fvy4Xn/9dbeBp4899pjWr1+v9957Tz/99JPatWun8PBwHT58WDt27FBsbKzmz59/xTfjrF69uiZPnqyhQ4fqhhtuUI8ePRQUFKQvv/xSO3fuVJcuXdy+QXSlBg8enOW3+F599VV17NhRMTExGjdunK699lp17NhRUVFROn78uPbs2aPVq1fr5ZdfVq1atSRJTz31lJ566ik1bNhQ3bt314ULF7R06VIZY1S/fv0sv7XWu3dvvfnmmxo9erT186WcTqf++9//6o477lCrVq102223qV69enI4HNq3b59Wr16tcuXKWQOcJ0+erBUrVqhly5aKjo5WQECANm/erGXLlqlatWrq1q1bfnQhigqvfhEdyAcZrx+T1SPjNVSSk5PNlClTzI033miCgoJMYGCgiY6ONl27djXvv/++23VTjhw5Yh566CETHh5uAgICTL169cyMGTPMX3/95fG6LVFRUSYqKirbOvNyrZeMXNeayXgdGZcVK1YYSWbMmDGZ5r355pumVq1apkSJEqZKlSrmn//8p4mPjzeSTJcuXTzWdKmM17nxZMKECVafe6pvxowZpkaNGsbf399ERkaa0aNHm3PnzuV4312y62dP63K1P3HihHn00UdNhQoVjNPpNPXr1zfz58/Pcn8XLVpk2rVrZ0JDQ42/v7+pXLmyad26tZk6dao5evSo1c51nZsVK1Zkua7sLF682LRq1cqULl3aOJ1OU69ePTN16lS356LLlVznJrtHXFyc1X7p0qXmrrvuMtdcc43x9/c3FStWNM2aNTPjxo1zux5Penq6mTVrlqlTp44JCAgwFStWNA8//LA5cuTIZeusUaOGkWSqVKli0tLSsmx34MABM3jwYFOjRg3jdDpNcHCwqVWrlhkwYIBZtmyZ1e7bb781ffr0Mddff70pXbq0KVWqlKldu7YZNWqU2+8KxYPDGA/H5gEUCz/88INuv/12Pffccx6/9gwARRFjboBi4OjRo5m+Zn7y5ElrvEvGW04AQFHHmBugGJg3b55effVV3XbbbQoPD9ehQ4f07bff6siRI+rXr5/Hi9sBQFFFuAGKgVtuucW6m/aJEyfk6+urWrVqKSYmRk8++aS3ywOAfMWYGwAAYCuMuQEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALZCuAEAALby/wA7XpjV3gKQdwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# number of leaves domain\n",
        "plt.hist(param_grid['num_leaves'], color = 'm', edgecolor = 'k')\n",
        "plt.xlabel('Learning Number of Leaves', size = 14); plt.ylabel('Count', size = 14); plt.title('Number of Leaves Distribution', size = 18);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AErAgwEO2uCm"
      },
      "source": [
        "### Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIKeH-6w2xkr"
      },
      "source": [
        "Although we don't generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random.\n",
        "\n",
        "We will implement these algorithms as soon as we cover the final part of hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuuekjXs261T"
      },
      "source": [
        "### Results History"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25o92fNT2-cz"
      },
      "source": [
        "The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "M12c6Nue2vH0"
      },
      "outputs": [],
      "source": [
        "# Dataframes for random and grid search\n",
        "random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
        "                              index = list(range(MAX_EVALS)))\n",
        "\n",
        "grid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
        "                              index = list(range(MAX_EVALS)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBkrzOvl3BPM"
      },
      "source": [
        "### Grid Search Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euTbBraN3JXT",
        "outputId": "9a1bc6d1-586d-4507-a413-5e11d4358091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2620800000000000 combinations\n"
          ]
        }
      ],
      "source": [
        "com = 1\n",
        "for x in param_grid.values():\n",
        "    com *= len(x)\n",
        "print('There are {} combinations'.format(com))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgbArKMX3NNr",
        "outputId": "cfd52ee5-3d76-4b65-c46b-5347917e7da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This would take 8310502283 years to finish.\n"
          ]
        }
      ],
      "source": [
        "print('This would take {:.0f} years to finish.'.format((100 * com) / (60 * 60 * 24 * 365)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nuGhVtUJ3NyD"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def grid_search(param_grid, max_evals = MAX_EVALS):\n",
        "    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n",
        "\n",
        "    # Dataframe to store results\n",
        "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
        "                              index = list(range(MAX_EVALS)))\n",
        "\n",
        "    keys, values = zip(*param_grid.items())\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    # Iterate through every possible combination of hyperparameters\n",
        "    for v in itertools.product(*values):\n",
        "\n",
        "        # Create a hyperparameter dictionary\n",
        "        hyperparameters = dict(zip(keys, v))\n",
        "\n",
        "        # Set the subsample ratio accounting for boosting type\n",
        "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
        "\n",
        "        # Evalute the hyperparameters\n",
        "        eval_results = objective(hyperparameters, i)\n",
        "\n",
        "        results.loc[i, :] = eval_results\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        # Normally would not limit iterations\n",
        "        if i > MAX_EVALS:\n",
        "            break\n",
        "\n",
        "    # Sort with best score on top\n",
        "    results.sort_values('score', ascending = False, inplace = True)\n",
        "    results.reset_index(inplace = True)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a5z_b7L3TKs",
        "outputId": "b1c2d962-84ee-48f1-de7b-dbce3e253a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013157 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009655 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009289 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065832 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[524]\tcv_agg's valid auc: 0.727682 + 0.0220251\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004204 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004039 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012559 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003828 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004087 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[524]\tcv_agg's valid auc: 0.727682 + 0.0220251\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004498 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005407 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004142 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013089 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003837 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[524]\tcv_agg's valid auc: 0.727682 + 0.0220251\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003829 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010947 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003884 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004514 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003896 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[524]\tcv_agg's valid auc: 0.727682 + 0.0220251\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004894 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004087 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003844 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003900 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010988 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[524]\tcv_agg's valid auc: 0.727682 + 0.0220251\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004422 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004029 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003865 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003941 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006808 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 93\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[524]\tcv_agg's valid auc: 0.727682 + 0.0220251\n",
            "The best validation score was 0.72768\n",
            "\n",
            "The best hyperparameters were:\n",
            "{'boosting_type': 'gbdt',\n",
            " 'colsample_bytree': 0.6,\n",
            " 'is_unbalance': True,\n",
            " 'learning_rate': 0.004999999999999999,\n",
            " 'min_child_samples': 20,\n",
            " 'n_estimators': 524,\n",
            " 'num_leaves': 20,\n",
            " 'reg_alpha': 0.0,\n",
            " 'reg_lambda': 0.0,\n",
            " 'subsample': 0.5,\n",
            " 'subsample_for_bin': 20000}\n"
          ]
        }
      ],
      "source": [
        "grid_results = grid_search(param_grid)\n",
        "\n",
        "print('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\n",
        "print('\\nThe best hyperparameters were:')\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(grid_results.loc[0, 'params'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz-AIIcS3X9M",
        "outputId": "08e73156-2aac-4f0e-a51a-90211bcee268"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 824, number of negative: 9176\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008550 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9997\n",
            "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 93\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.082400 -> initscore=-2.410176\n",
            "[LightGBM] [Info] Start training from score -2.410176\n",
            "The best model from grid search scores 0.73066 ROC AUC on the test set.\n"
          ]
        }
      ],
      "source": [
        "# Get the best parameters\n",
        "grid_search_params = grid_results.loc[0, 'params']\n",
        "\n",
        "# Create, train, test model\n",
        "model = lgb.LGBMClassifier(**grid_search_params, random_state=42)\n",
        "model.fit(train_features, train_labels)\n",
        "\n",
        "preds = model.predict_proba(test_features)[:, 1]\n",
        "\n",
        "print('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKMy9F7Q3C5z"
      },
      "source": [
        "### Random Search Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbWY8Lml3ba7",
        "outputId": "39474127-fb98-4a1f-e844-1abeef6304a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'boosting_type': 'goss',\n",
              " 'num_leaves': 88,\n",
              " 'learning_rate': 0.027778881111994384,\n",
              " 'subsample_for_bin': 220000,\n",
              " 'min_child_samples': 175,\n",
              " 'reg_alpha': 0.8979591836734693,\n",
              " 'reg_lambda': 0.6122448979591836,\n",
              " 'colsample_bytree': 0.8222222222222222,\n",
              " 'subsample': 1.0,\n",
              " 'is_unbalance': False}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random.seed(50)\n",
        "\n",
        "# Randomly sample from dictionary\n",
        "random_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
        "# Deal with subsample ratio\n",
        "random_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n",
        "\n",
        "random_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "YtClkrkd3dZj"
      },
      "outputs": [],
      "source": [
        "def random_search(param_grid, max_evals = 1):\n",
        "    \"\"\"Random search for hyperparameter optimization\"\"\"\n",
        "\n",
        "    # Dataframe for results\n",
        "    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
        "                                  index = list(range(1)))\n",
        "\n",
        "    # Keep searching until reach max evaluations\n",
        "    for i in range(1):\n",
        "\n",
        "        # Choose random hyperparameters\n",
        "        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
        "        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
        "\n",
        "        # Evaluate randomly selected hyperparameters\n",
        "        eval_results = objective(hyperparameters, i)\n",
        "\n",
        "        results.loc[i, :] = eval_results\n",
        "\n",
        "    # Sort with best score on top\n",
        "    results.sort_values('score', ascending = False, inplace = True)\n",
        "    results.reset_index(inplace = True)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2hQ-UJa3d30",
        "outputId": "a2064033-e821-4723-9ccd-81db6212cb6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025113 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9979\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 84\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018669 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9979\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 84\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004588 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9979\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 84\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010602 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9979\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 84\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004080 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 9979\n",
            "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 84\n",
            "[LightGBM] [Warning] Unknown parameter: importance_type\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Start training from score 0.082375\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Start training from score 0.082500\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[7]\tcv_agg's valid auc: 0.705295 + 0.0238222\n",
            "The best validation score was 0.70530\n",
            "\n",
            "The best hyperparameters were:\n",
            "{'boosting_type': 'gbdt',\n",
            " 'colsample_bytree': 0.8222222222222222,\n",
            " 'is_unbalance': True,\n",
            " 'learning_rate': 0.40260198354127363,\n",
            " 'min_child_samples': 240,\n",
            " 'n_estimators': 7,\n",
            " 'num_leaves': 41,\n",
            " 'reg_alpha': 0.12244897959183673,\n",
            " 'reg_lambda': 0.44897959183673464,\n",
            " 'subsample': 0.6414141414141414,\n",
            " 'subsample_for_bin': 60000}\n"
          ]
        }
      ],
      "source": [
        "random_results = random_search(param_grid)\n",
        "\n",
        "print('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\n",
        "print('\\nThe best hyperparameters were:')\n",
        "\n",
        "import pprint\n",
        "pprint.pprint(random_results.loc[0, 'params'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpT2CzbN3gi8",
        "outputId": "c7edfea7-0267-4c11-a942-147c8675a609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Number of positive: 824, number of negative: 9176\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026986 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 9979\n",
            "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 84\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.082400 -> initscore=-2.410176\n",
            "[LightGBM] [Info] Start training from score -2.410176\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "The best model from random search scores 0.70502 ROC AUC on the test set.\n"
          ]
        }
      ],
      "source": [
        "# Get the best parameters\n",
        "random_search_params = random_results.loc[0, 'params']\n",
        "\n",
        "# Create, train, test model\n",
        "model = lgb.LGBMClassifier(**random_search_params, random_state = 42)\n",
        "model.fit(train_features, train_labels)\n",
        "\n",
        "preds = model.predict_proba(test_features)[:, 1]\n",
        "\n",
        "print('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5hVyTWEH_VP"
      },
      "source": [
        "## Optimization Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpiR_tCbIip9"
      },
      "source": [
        "Until now, you have probably always used Gradient Descent to update the parameters and minimize the cost. In this part of the lab, we are going to learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKs7eXt0IwCi"
      },
      "source": [
        "To get started, run the following code to import the libraries you will need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "kHcqSzVcIw2q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import math\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "\n",
        "from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
        "from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
        "from testCases import *\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0rfemWsIzms"
      },
      "source": [
        "### Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw4FLwRbI33D"
      },
      "source": [
        "A simple optimization method in machine learning is gradient descent (GD). When you take gradient steps with respect to all\n",
        " examples on each step, it is also called Batch Gradient Descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyFMby8bJ0RW"
      },
      "source": [
        "Implement the gradient descent update rule. The gradient descent rule is, for l = 1, ..., L\n",
        "\n",
        "W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{1}\n",
        "b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{2}\n",
        "\n",
        "where L is the number of layers and \\alpha is the learning rate. All parameters should be stored in the parameters dictionary. Note that the iterator l starts at 0 in the for loop while the first parameters are W^{[1]} and b^{[1]}. You need to shift l to l+1 when coding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9eZJhMqiiiU"
      },
      "source": [
        "###Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aIvELj4CI1Aj"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: update_parameters_with_gd\n",
        "\n",
        "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using one step of gradient descent\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters to be updated:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients to update each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    for l in range(L):\n",
        "\n",
        "        # ***********************************************\n",
        "        # TASK 1 - IMPLEMENT GRADIENT DESCENT UPDATE RULE\n",
        "        # ***********************************************\n",
        "\n",
        "        # WRITE YOUR CODE HERE\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]  - learning_rate * grads[\"db\" + str(l+1)]\n",
        "\n",
        "        # ***********************************************\n",
        "        # ***********************************************\n",
        "\n",
        "    return parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7RKSnULKiby",
        "outputId": "6c2209ef-7389-46eb-dc6e-e9a7f1f20207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[ 1.63535156 -0.62320365 -0.53718766]\n",
            " [-1.07799357  0.85639907 -2.29470142]]\n",
            "b1 = [[ 1.74604067]\n",
            " [-0.75184921]]\n",
            "W2 = [[ 0.32171798 -0.25467393  1.46902454]\n",
            " [-2.05617317 -0.31554548 -0.3756023 ]\n",
            " [ 1.1404819  -1.09976462 -0.1612551 ]]\n",
            "b2 = [[-0.88020257]\n",
            " [ 0.02561572]\n",
            " [ 0.57539477]]\n"
          ]
        }
      ],
      "source": [
        "parameters, grads, learning_rate = update_parameters_with_gd_test_case()\n",
        "\n",
        "parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw6tXsdGKkib"
      },
      "source": [
        "A variant of this is Stochastic Gradient Descent (SGD), which is equivalent to mini-batch gradient descent where each mini-batch has just 1 example. The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul9S3DxFKrD8"
      },
      "source": [
        "**Batch Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obC_vAUWKx6r"
      },
      "outputs": [],
      "source": [
        "# X = data_input\n",
        "# Y = labels\n",
        "# parameters = initialize_parameters(layers_dims)\n",
        "# for i in range(0, num_iterations):\n",
        "#     # Forward propagation\n",
        "#     a, caches = forward_propagation(X, parameters)\n",
        "#     # Compute cost.\n",
        "#     cost = compute_cost(a, Y)\n",
        "#     # Backward propagation.\n",
        "#     grads = backward_propagation(a, caches, parameters)\n",
        "#     # Update parameters.\n",
        "#     parameters = update_parameters(parameters, grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnAqX1pmKuUk"
      },
      "source": [
        "**Stochastic Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3Hh8HNEK0W9"
      },
      "outputs": [],
      "source": [
        "# X = data_input\n",
        "# Y = labels\n",
        "# parameters = initialize_parameters(layers_dims)\n",
        "# for i in range(0, num_iterations):\n",
        "#     for j in range(0, m):\n",
        "#         # Forward propagation\n",
        "#         a, caches = forward_propagation(X[:,j], parameters)\n",
        "#         # Compute cost\n",
        "#         cost = compute_cost(a, Y[:,j])\n",
        "#         # Backward propagation\n",
        "#         grads = backward_propagation(a, caches, parameters)\n",
        "#         # Update parameters.\n",
        "#         parameters = update_parameters(parameters, grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nELz0sQHK9EY"
      },
      "source": [
        "### Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX4dM7yuLAJj"
      },
      "source": [
        "First, let us learn how to build mini-batches from the training set (X, Y).\n",
        "\n",
        "There are 2 steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E_cCV_xLFlD"
      },
      "source": [
        "**Shuffle**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc966olELItD"
      },
      "source": [
        "Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the i^{th} column of X is the example corresponding to the i^{th} label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJVZYS0RLG2K"
      },
      "source": [
        "**Partition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH7O1DJ_LNem"
      },
      "source": [
        "Partition the shuffled (X, Y) into mini-batches of size mini_batch_size (here 64). Note that the number of training examples is not always divisible by mini_batch_size. The last mini batch might be smaller, but you don't need to worry about this. When the final mini-batch is smaller than the full mini_batch_size, it will look like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-OYLDcULVwt"
      },
      "source": [
        "Implement random_mini_batches. We coded the shuffling part for you. To help you with the partitioning step, we give you the following code that selects the indexes for the 1^{st} and 2^{nd} mini-batches:\n",
        "\n",
        "\n",
        "```\n",
        "first_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]\n",
        "second_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]\n",
        "```\n",
        "\n",
        "Note that the last mini-batch might end up smaller than mini_batch_size=64. Let \\lfloor s \\rfloor represents s rounded down to the nearest integer (this is math.floor(s) in Python). If the total number of examples is not a multiple of mini_batch_size=64 then there will be \\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor mini-batches with a full 64 examples, and the number of examples in the final mini-batch will be (m-mini_\\_batch_\\_size \\times \\lfloor \\frac{m}{mini\\_batch\\_size}\\rfloor)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWLA2t5disAa"
      },
      "source": [
        "###Task 2 and 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7caFaNciLtsD"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: random_mini_batches\n",
        "\n",
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "    \"\"\"\n",
        "    Creates a list of random minibatches from (X, Y)\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data, of shape (input size, number of examples)\n",
        "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
        "    mini_batch_size -- size of the mini-batches, integer\n",
        "\n",
        "    Returns:\n",
        "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "\n",
        "    # Step 1: Shuffle (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
        "\n",
        "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "\n",
        "        # *******************************\n",
        "        # TASK 2 - IMPLEMENT PARTITIONING\n",
        "        # *******************************\n",
        "\n",
        "        # WRITE YOUR CODE HERE\n",
        "        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "\n",
        "        # *******************************\n",
        "        # *******************************\n",
        "\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "\n",
        "        # ************************\n",
        "        # TASK 3 - HANDLE END CASE\n",
        "        # ************************\n",
        "\n",
        "        # WRITE YOUR CODE HERE\n",
        "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size:]\n",
        "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size:]\n",
        "\n",
        "        # ************************\n",
        "        # ************************\n",
        "\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    return mini_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEVtFo7ZMI4O",
        "outputId": "c6933ccb-d265-4196-e53c-b7ed6bce01c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of the 1st mini_batch_X: (12288, 64)\n",
            "shape of the 2nd mini_batch_X: (12288, 64)\n",
            "shape of the 3rd mini_batch_X: (12288, 20)\n",
            "shape of the 1st mini_batch_Y: (1, 64)\n",
            "shape of the 2nd mini_batch_Y: (1, 64)\n",
            "shape of the 3rd mini_batch_Y: (1, 20)\n",
            "mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n"
          ]
        }
      ],
      "source": [
        "X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\n",
        "mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
        "\n",
        "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
        "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
        "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
        "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
        "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape))\n",
        "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
        "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJQSc49LMLzz"
      },
      "source": [
        "### Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l_vv3_yMR2j"
      },
      "source": [
        "Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvRnIgs9MpUF"
      },
      "source": [
        "Initialize the velocity. The velocity, v, is a python dictionary that needs to be initialized with arrays of zeros. Its keys are the same as those in the grads dictionary, that is: for l =1,...,L:\n",
        "\n",
        "```\n",
        "v[\"dW\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"W\" + str(l+1)])\n",
        "v[\"db\" + str(l+1)] = ... #(numpy array of zeros with the same shape as parameters[\"b\" + str(l+1)])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-vY9n56iw9S"
      },
      "source": [
        "###Task 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4cU5dUscMOxa"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: initialize_velocity\n",
        "\n",
        "def initialize_velocity(parameters):\n",
        "    \"\"\"\n",
        "    Initializes the velocity as a python dictionary with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "\n",
        "    Returns:\n",
        "    v -- python dictionary containing the current velocity.\n",
        "                    v['dW' + str(l)] = velocity of dWl\n",
        "                    v['db' + str(l)] = velocity of dbl\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "\n",
        "    # Initialize velocity\n",
        "    for l in range(L):\n",
        "\n",
        "        # ****************************\n",
        "        # TASK 4 - INITIALIZE VELOCITY\n",
        "        # ****************************\n",
        "\n",
        "        # WRITE YOUR CODE HERE\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
        "\n",
        "        # ****************************\n",
        "        # ****************************\n",
        "\n",
        "    return v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v4GhlOAM51V",
        "outputId": "bee2f270-76d5-473d-9427-f46d0f8a330c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v[\"dW1\"] = [[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db1\"] = [[0.]\n",
            " [0.]]\n",
            "v[\"dW2\"] = [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db2\"] = [[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        "parameters = initialize_velocity_test_case()\n",
        "\n",
        "v = initialize_velocity(parameters)\n",
        "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DADSipSri4Zb"
      },
      "source": [
        "###Task 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "dsqUgIbPNLXD"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: update_parameters_with_momentum\n",
        "\n",
        "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using Momentum\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- python dictionary containing the current velocity:\n",
        "                    v['dW' + str(l)] = ...\n",
        "                    v['db' + str(l)] = ...\n",
        "    beta -- the momentum hyperparameter, scalar\n",
        "    learning_rate -- the learning rate, scalar\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    v -- python dictionary containing your updated velocities\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "\n",
        "    # Momentum update for each parameter\n",
        "    for l in range(L):\n",
        "\n",
        "        # ************************\n",
        "        # TASK 5 - MOMENTUM UPDATE\n",
        "        # ************************\n",
        "\n",
        "        # WRITE YOUR CODE HERE\n",
        "\n",
        "        # COMPUTE VELOCITIES\n",
        "        v[\"dW\" + str(l+1)] = beta * v[\"dW\" + str(l+1)] + (1 - beta) * grads[\"dW\" + str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta * v[\"db\" + str(l+1)] + (1 - beta) * grads[\"db\" + str(l+1)]\n",
        "\n",
        "        # UPDATE PARAMETERS\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v[\"db\" + str(l+1)]\n",
        "\n",
        "        # ************************\n",
        "        # ************************\n",
        "\n",
        "    return parameters, v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOnAY1ahNOlD",
        "outputId": "89748ad7-2bb4-4dff-c786-072f679ca242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[ 1.62544598 -0.61290114 -0.52907334]\n",
            " [-1.07347112  0.86450677 -2.30085497]]\n",
            "b1 = [[ 1.74493465]\n",
            " [-0.76027113]]\n",
            "W2 = [[ 0.31930698 -0.24990073  1.4627996 ]\n",
            " [-2.05974396 -0.32173003 -0.38320915]\n",
            " [ 1.13444069 -1.0998786  -0.1713109 ]]\n",
            "b2 = [[-0.87809283]\n",
            " [ 0.04055394]\n",
            " [ 0.58207317]]\n",
            "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
            " [ 0.05024943  0.09008559 -0.06837279]]\n",
            "v[\"db1\"] = [[-0.01228902]\n",
            " [-0.09357694]]\n",
            "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
            " [-0.03967535 -0.06871727 -0.08452056]\n",
            " [-0.06712461 -0.00126646 -0.11173103]]\n",
            "v[\"db2\"] = [[0.02344157]\n",
            " [0.16598022]\n",
            " [0.07420442]]\n"
          ]
        }
      ],
      "source": [
        "parameters, grads, v = update_parameters_with_momentum_test_case()\n",
        "\n",
        "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trGUbWOjNQmi"
      },
      "source": [
        "### Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OTvRr8kMTEC"
      },
      "source": [
        "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5JQSS0hxNXb1"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: initialize_adam\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\"\n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "\n",
        "    Returns:\n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
        "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
        "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
        "\n",
        "    return v, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyS1EbkdNYsE",
        "outputId": "72eb8dc7-7920-483b-8b64-fb44c3a92849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v[\"dW1\"] = [[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db1\"] = [[0.]\n",
            " [0.]]\n",
            "v[\"dW2\"] = [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db2\"] = [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "s[\"dW1\"] = [[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "s[\"db1\"] = [[0.]\n",
            " [0.]]\n",
            "s[\"dW2\"] = [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "s[\"db2\"] = [[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ],
      "source": [
        "parameters = initialize_adam_test_case()\n",
        "\n",
        "v, s = initialize_adam(parameters)\n",
        "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
        "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
        "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
        "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
        "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SxqIAJkDNb5i"
      },
      "outputs": [],
      "source": [
        "# FUNCTION: update_parameters_with_adam\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates\n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates\n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "\n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads['dW' + str(l+1)]\n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads['db' + str(l+1)]\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - math.pow(beta1,t))\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - math.pow(beta1,t))\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        s[\"dW\" + str(l+1)] = beta1 * s[\"dW\" + str(l+1)] + (1 - beta2) * grads['dW' + str(l+1)] * grads['dW' + str(l+1)]\n",
        "        s[\"db\" + str(l+1)] = beta1 * s[\"db\" + str(l+1)] + (1 - beta2) * grads['db' + str(l+1)] * grads['db' + str(l+1)]\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - math.pow(beta2,t))\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - math.pow(beta2,t))\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / (np.square(s_corrected[\"dW\" + str(l+1)])+ epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / (np.square(s_corrected[\"db\" + str(l+1)])+ epsilon)\n",
        "\n",
        "    return parameters, v, s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SAAEomwNdMl",
        "outputId": "26ad96d3-9046-452b-c6af-49d81f8fd631"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1 = [[ 1.64012005 -0.62577712 -0.55686923]\n",
            " [-1.23872803  0.83663988 -2.23573927]]\n",
            "b1 = [[13.07517618]\n",
            " [-0.73554047]]\n",
            "W2 = [[ 1.41301675e+00 -3.90354632e-01  1.52566920e+00]\n",
            " [-1.72338971e+00 -2.57602399e-01 -3.49221843e-01]\n",
            " [ 1.20330813e+00  4.05392097e+03 -1.57349972e-01]]\n",
            "b2 = [[-2.51055757]\n",
            " [ 0.03761433]\n",
            " [ 0.53134184]]\n",
            "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
            " [ 0.05024943  0.09008559 -0.06837279]]\n",
            "v[\"db1\"] = [[-0.01228902]\n",
            " [-0.09357694]]\n",
            "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
            " [-0.03967535 -0.06871727 -0.08452056]\n",
            " [-0.06712461 -0.00126646 -0.11173103]]\n",
            "v[\"db2\"] = [[0.02344157]\n",
            " [0.16598022]\n",
            " [0.07420442]]\n",
            "s[\"dW1\"] = [[0.00121136 0.00131039 0.00081287]\n",
            " [0.0002525  0.00081154 0.00046748]]\n",
            "s[\"db1\"] = [[1.51020075e-05]\n",
            " [8.75664434e-04]]\n",
            "s[\"dW2\"] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]\n",
            " [1.57413361e-04 4.72206320e-04 7.14372576e-04]\n",
            " [4.50571368e-04 1.60392066e-07 1.24838242e-03]]\n",
            "s[\"db2\"] = [[5.49507194e-05]\n",
            " [2.75494327e-03]\n",
            " [5.50629536e-04]]\n"
          ]
        }
      ],
      "source": [
        "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
        "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
        "\n",
        "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
        "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
        "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
        "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
        "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGtUes_AfxfC"
      },
      "source": [
        "##Conclusion of tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CWIWtSahPrb"
      },
      "source": [
        "### **Conclusion of Task-1: Gradient Descent Parameter Update**  \n",
        "\n",
        "In this task, we implemented the **gradient descent update rule** to adjust the parameters of a neural network.The update is applied iteratively for all layers in the network, ensuring that the model minimizes the loss function. The correctness of the implementation is verified by printing updated parameter values after applying the function. This step is fundamental in optimizing deep learning models using gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzEVhBGiQOE"
      },
      "source": [
        "### **Conclusion of Tasks 2 & 3: Mini-Batch Partitioning for Gradient Descent**\n",
        "\n",
        "In this task, we implemented the process of **creating random mini-batches** from the given dataset. This is essential for **stochastic gradient descent (SGD) and mini-batch gradient descent**, which improve training efficiency and help avoid local minima.\n",
        "\n",
        "The mini-batch partitioning follows these steps:\n",
        "\n",
        "1. **Shuffling the Data:**  \n",
        "   To ensure randomness in training, the dataset is shuffled using a random permutation.\n",
        "\n",
        "2. **Creating Complete Mini-Batches:**  \n",
        "   The dataset is then split into mini-batches of the size 64.\n",
        "\n",
        "3. **Handling the Last Mini-Batch (if the total samples are not divisible by batch size):**  \n",
        "   If there are leftover samples that do not fit into a full batch, they are grouped into a smaller final batch.\n",
        "\n",
        "   This ensures that all training data is utilized effectively.\n",
        "\n",
        "### **Our understanding:**\n",
        "1. **Shuffling helps prevent biases in training order.**  \n",
        "2. **Mini-batching speeds up training compared to full-batch gradient descent.**  \n",
        "3. **Handling incomplete batches ensures that no data is lost.**  \n",
        "4. **This method improves model generalization and stability during optimization.**  \n",
        "\n",
        "By implementing this approach, we create an efficient **mini-batch gradient descent mechanism**, making the training process faster and more robust.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVVQ34sbiWVf"
      },
      "source": [
        "### **Conclusion of Tasks 4 & 5: Parameter Update Using Momentum**  \n",
        "\n",
        "In this task, we implemented the **momentum-based gradient update**, an optimization technique that accelerates gradient descent by maintaining an exponentially weighted average of past gradients.  \n",
        "\n",
        "The key steps involved:  \n",
        "\n",
        "1. **Computing the Velocity:**  \n",
        "   The velocity update equation smooths out gradient fluctuations, preventing sharp changes. **β** is momentum hyperparameter determines how much past gradients influence the current update.\n",
        "\n",
        "2. **Updating the Parameters:**  \n",
        "   The parameters are then updated using the momentum-adjusted gradients\n",
        "\n",
        "   \n",
        "\n",
        "    \n",
        "\n",
        "### **Our understanding:**  \n",
        "1. **Momentum reduces oscillations** in gradient descent, leading to faster convergence.  \n",
        "2. **Smooths updates** by incorporating past gradient information, making training more stable.  \n",
        "3. **Particularly effective in deep networks** where standard gradient descent might be slow.  \n",
        "\n",
        "By implementing momentum-based optimization, we enhance **gradient descent efficiency**, making learning faster and more reliable.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
